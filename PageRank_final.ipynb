{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY0c7z3kOi-K"
      },
      "source": [
        "# PageRank 팀 코드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4CPw-8aOQFT"
      },
      "source": [
        "### **Install Pyspark**   \n",
        "just implement all blocks in this section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqSRhf5HKxAT"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hifOPhlzK1y6"
      },
      "outputs": [],
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOnScK8sK5Y6"
      },
      "outputs": [],
      "source": [
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqb_28AdK7wS"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWr_eSihLPHi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAlygjekOn94"
      },
      "source": [
        "### **Upload data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfehzSwVKfnn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzAKyk4IQNZK"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SoyIYPwPgSh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "#modify as your filename\n",
        "filename = 'data_21_22.csv'\n",
        "\n",
        "df = pd.read_csv(filename)\n",
        "#df = df[['cino', 'compound_nouns', 'only_weight_dic', 'use_less_dic']]\n",
        "#preview for data\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rXCW3RKQvv4"
      },
      "outputs": [],
      "source": [
        "df = df[['cino', 'compound_nouns', 'only_weight_dic', 'use_less_dic']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_ro9XD8aXme"
      },
      "source": [
        "#### replace nan and convert data form to list\n",
        "*if df['compound_nouns']'s list length is 0, drop that row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FhlYlykuT1OW"
      },
      "outputs": [],
      "source": [
        "#fill 'nan' as str of empty list: '[]'\n",
        "df['compound_nouns'] = df['compound_nouns'].fillna('[]')\n",
        "\n",
        "#make a list for text had to be processed\n",
        "#'lists' used as saving the 'compound_nouns' list on df, 'n_lists' used as a corpus for computing tfidf\n",
        "lists = []\n",
        "for element in df['compound_nouns']:\n",
        "  noun = ast.literal_eval(element)\n",
        "  lists.append(noun)\n",
        "\n",
        "df['compound_nouns'] = lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9SLSfm1W_ZI"
      },
      "outputs": [],
      "source": [
        "#fill 'nan' as str of empty list: '[]'\n",
        "df['only_weight_dic'] = df['only_weight_dic'].fillna('[]')\n",
        "\n",
        "#make a list for text had to be processed\n",
        "#'lists' used as saving the 'compound_nouns' list on df, 'n_lists' used as a corpus for computing tfidf\n",
        "lists = []\n",
        "for element in df['only_weight_dic']:\n",
        "  noun = ast.literal_eval(element)\n",
        "  lists.append(noun)\n",
        "\n",
        "df['only_weight_dic'] = lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ih1Lt9YqW_Io"
      },
      "outputs": [],
      "source": [
        "#fill 'nan' as str of empty list: '[]'\n",
        "df['use_less_dic'] = df['use_less_dic'].fillna('[]')\n",
        "\n",
        "#make a list for text had to be processed\n",
        "#'lists' used as saving the 'compound_nouns' list on df, 'n_lists' used as a corpus for computing tfidf\n",
        "lists = []\n",
        "for element in df['use_less_dic']:\n",
        "  noun = ast.literal_eval(element)\n",
        "  lists.append(noun)\n",
        "\n",
        "df['use_less_dic'] = lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHxRWCgxzZtq"
      },
      "outputs": [],
      "source": [
        "#if df['compound_nouns']'s list length is 0, drop that row\n",
        "lists = []\n",
        "for i in range(df.shape[0]):\n",
        "  if len(df['compound_nouns'][i]) == 0:\n",
        "    print(i)\n",
        "    lists.append(i)\n",
        "df = df.drop(lists)\n",
        "df = df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWkpI48Uay3_"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hJBanW6UANa"
      },
      "source": [
        "### **PageRank**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWY9WsqcUv6B"
      },
      "source": [
        "#### Import the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6a6zwr3LSMq"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "sc = SparkContext(\"local\", \"pagerank\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2QB9xptLbHS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWrsqnamU0rp"
      },
      "source": [
        "#### Find class type: df['cino_alpha']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iyw75P0T1OV"
      },
      "outputs": [],
      "source": [
        "#take str part from cino\n",
        "df['cino_alpha'] = df['cino'].str[0:3]\n",
        "df['cino_alpha']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8tvxgWdVYUJ"
      },
      "source": [
        "#### Compute Tfidf score: df['tfidf_result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo_S_77E3_fa"
      },
      "outputs": [],
      "source": [
        "n_lists = []\n",
        "for element in df['compound_nouns']:\n",
        "  n_lists.append(' '.join(element))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B7P0vKyT1OX"
      },
      "outputs": [],
      "source": [
        "#compute the tfidf score\n",
        "vectorizer = TfidfVectorizer()\n",
        "sp_matrix = vectorizer.fit_transform(n_lists)\n",
        "tfidf_result = []\n",
        "\n",
        "word2id = defaultdict(lambda : 0)\n",
        "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
        "  word2id[feature] = idx\n",
        "\n",
        "for i, sent in enumerate(n_lists):\n",
        "  k = [ (token, sp_matrix[i, word2id[token]]) for token in sent.split() ]\n",
        "  tfidf_result.append(k)\n",
        "\n",
        "#save the tfidf score\n",
        "df['tfidf_result'] = tfidf_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSmLPOq0NS-M"
      },
      "source": [
        "#### Sum tfidf scores: df['tfidf_result_sum']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Cz97nfwET1OX"
      },
      "outputs": [],
      "source": [
        "#sum the score of same token\n",
        "lists = []\n",
        "for element in df['tfidf_result']:\n",
        "  tf_rdd = sc.parallelize(element)\n",
        "  lists.append(tf_rdd.reduceByKey(lambda a, b: a+b).collect())\n",
        "\n",
        "df['tfidf_result_sum'] = lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqKzjb2BM-Sc"
      },
      "source": [
        "#### Apply df['only_weight_dic'] and df['use_less_dic']: df['update_result']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lGsUBWi-MfZQ"
      },
      "outputs": [],
      "source": [
        "#tfidf_sum_list: 'tfidf_result_sum' of one row\n",
        "#weight_list: (token, weight) lists of compound_nouns\n",
        "#3 kinds of weight: weight_p, weight_m, 1\n",
        "def update (tfidf_sum_list, weight_list):\n",
        "    results = []\n",
        "    for i in range(len(tfidf_sum_list)):\n",
        "      temp = list(tfidf_sum_list[i])\n",
        "      temp[1] = temp[1] * weight_list[i][1]\n",
        "      token_result = tuple(temp)\n",
        "      results.append(token_result)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WKibzKysMfZQ"
      },
      "outputs": [],
      "source": [
        "#define weight\n",
        "weight_p = 10\n",
        "weight_m = 0.0001\n",
        "\n",
        "#make a list that has (token, weight) tuples from 'compound_nouns'\n",
        "update_lists = []\n",
        "for i in range(df.shape[0]):\n",
        "  w_lists = []\n",
        "  for token in df['compound_nouns'][i]:\n",
        "    if token in df['only_weight_dic'][i]:\n",
        "      w_lists.append((token, weight_p))\n",
        "    elif token in df['use_less_dic'][i]:\n",
        "      w_lists.append((token, weight_m))\n",
        "    else:\n",
        "      w_lists.append((token, 1))\n",
        "\n",
        "  update_lists.append(update(df['tfidf_result_sum'][i], w_lists))\n",
        "\n",
        "#save the update score of tfidf\n",
        "df['update_results'] = update_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n6oYTsiMKjWj"
      },
      "outputs": [],
      "source": [
        "sorted(df['update_results'][8], key =lambda t: t[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JonXWmVWVoQ5"
      },
      "source": [
        "#### Edit tfidf score with threshold: df['tfidf_edit']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2zL0i9rZ1QM9"
      },
      "outputs": [],
      "source": [
        "#define threshold\n",
        "threshold = 0.05\n",
        "zero = 0\n",
        "\n",
        "#change tfidf score which is lower than threshold as zero\n",
        "lists = []\n",
        "for element in df['update_results']:\n",
        "  tokens = []\n",
        "  for token in element:\n",
        "    token_l = list(token)\n",
        "    if token_l[1] < threshold:\n",
        "      token_l[1] = zero\n",
        "      token = tuple(token_l)\n",
        "    tokens.append(token)\n",
        "  lists.append(tokens)\n",
        "\n",
        "#save the edit version of tfidf\n",
        "df['tfidf_edit'] = lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1NY78LwWA25"
      },
      "source": [
        "#### K means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC6FF-UIZNSo"
      },
      "source": [
        "##### 1. Compute initial cluster with class type dictionary(alpha_dict): df['cino_alpha_index']\n",
        "##### 2. Find cino index of every row with cino dictionary(cino_dict): df['cino_index']\n",
        "##### 3. Find token index of every token with token dictionary(token_dict): df['token_index']\n",
        "##### 4. Make a location point of every token: df['kmeans_point']\n",
        "('cino_alpha_index', ['cino_index', 'token_index', 'tfidf_edit'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G19Dqq4YT1OZ"
      },
      "outputs": [],
      "source": [
        "#make a dictionary of 'cino alpha'\n",
        "lists = []\n",
        "lists = list(df.cino_alpha.unique())\n",
        "alpha_dict = {alpha : i for i,alpha in enumerate(lists)}\n",
        "\n",
        "#make a dictionary of 'cino'\n",
        "lists = []\n",
        "lists = list(df['cino'])\n",
        "cino_dict = {cino : i for i,cino in enumerate(lists)}\n",
        "\n",
        "#make a dictionary of 'compound_nouns'\n",
        "lists = []\n",
        "for element in df['compound_nouns']:\n",
        "  lists = lists + element\n",
        "lists = list(set(lists))\n",
        "token_dict = {token : i for i,token in enumerate(lists)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2psI9IXdT1Oa"
      },
      "outputs": [],
      "source": [
        "#compute initial cluster with class type dictionary\n",
        "lists = []\n",
        "for alpha in df['cino_alpha']:\n",
        "  lists.append(alpha_dict[alpha])\n",
        "df['cino_alpha_index'] = lists\n",
        "\n",
        "#find cino index of every row with class type dictionary\n",
        "lists = []\n",
        "for cino in df['cino']:\n",
        "  lists.append(cino_dict[cino])\n",
        "df['cino_index'] = lists\n",
        "\n",
        "#find token index of every token with token dictionary\n",
        "t_lists = []\n",
        "for element in df['compound_nouns']:\n",
        "  lists = []\n",
        "  for token in element:\n",
        "    lists.append(token_dict[token])\n",
        "  t_lists.append(lists)\n",
        "df['token_index'] = t_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA2ZjuCyaTSx"
      },
      "outputs": [],
      "source": [
        "#make a location point of every token ('kmeans_point')\n",
        "#(cluster, 'cino_index', 'token_index', 'tfidf_edit')\n",
        "k_lists = []\n",
        "for i in range(df.shape[0]):\n",
        "  lists = []\n",
        "  for k in range(len(df['tfidf_edit'][i])):\n",
        "    lists.append((df['cino_alpha_index'][i], np.array([df['cino_index'][i], df['token_index'][i][k], df['tfidf_edit'][i][k][1]])))\n",
        "  k_lists.append(lists)\n",
        "df['kmeans_point'] = k_lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWLqxZldWHSx"
      },
      "source": [
        "##### 5. Define functions\n",
        "    1) Compute distance between two points(compute_square_distance)   \n",
        "    2) Assign the points to nearest centroid(assign_to_nearest_centroid)   \n",
        "    3) Compute the difference between old_centroids and new_centroids(compute_metric)   \n",
        "    4) Cluster the data with k clusters(Kmeans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVxz7pcDT1Oa"
      },
      "outputs": [],
      "source": [
        "def compute_square_distance(a, b):\n",
        "    return np.sum((a - b)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEcANU9hT1Ob"
      },
      "outputs": [],
      "source": [
        "def assign_to_nearest_centroid(pair, centroids):\n",
        "    squared_distances = list(map(lambda centroid: compute_square_distance(pair[1], centroid[1]), centroids))\n",
        "    nearest_centroid_id = np.argmin(squared_distances)\n",
        "\n",
        "    return nearest_centroid_id, pair[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9YJSg-9T1Ob"
      },
      "outputs": [],
      "source": [
        "#difference between old_centroids and new_centroids\n",
        "def compute_metric(old_centroids, new_centroids):\n",
        "    old_centroids = sorted(old_centroids, key=lambda item: item[0])\n",
        "    new_centroids = sorted(new_centroids, key=lambda item: item[0])\n",
        "\n",
        "    distances = map(\n",
        "        lambda zipped: np.sqrt(compute_square_distance(zipped[0][1], zipped[1][1])),\n",
        "        zip(old_centroids, new_centroids)\n",
        "    )\n",
        "    sum_of_distances = sum(distances)\n",
        "\n",
        "    return sum_of_distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daekleRAT1Ob"
      },
      "outputs": [],
      "source": [
        "def Kmeans(datalist):\n",
        "    k_rdd = sc.parallelize(datalist)\n",
        "    centroids1 = k_rdd.groupByKey().mapValues(lambda value: np.mean(list(value), axis=0)*(1/3)).collect()\n",
        "    centroids2 = k_rdd.groupByKey().mapValues(lambda value: np.mean(list(value), axis=0)*(2/3)).map(lambda t: (2*t[0], t[1])).collect()\n",
        "    centroids = centroids1 + centroids2\n",
        "\n",
        "    for i in range(MAX_ITER):\n",
        "        print(i)\n",
        "        centroids_bc = sc.broadcast(centroids)\n",
        "\n",
        "        new_rdd = k_rdd.map(lambda pair: assign_to_nearest_centroid(pair, centroids_bc.value))\n",
        "        new_centroids_rdd = new_rdd.groupByKey().mapValues(lambda value: np.mean(list(value), axis=0)).cache()\n",
        "        new_centroids = new_centroids_rdd.collect()\n",
        "\n",
        "        update_centroids = {centroid[0] : centroid[1][1] for centroid in enumerate(centroids)}\n",
        "        for centroid in new_centroids:\n",
        "          update_centroids[centroid[0]] = centroid[1]\n",
        "        update_centroids = list(update_centroids.items())\n",
        "\n",
        "        metric = compute_metric(centroids, update_centroids)\n",
        "        print(metric)\n",
        "        if metric < THRESHOLD:\n",
        "            break\n",
        "\n",
        "        centroids = update_centroids\n",
        "\n",
        "    return new_rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWhULD1JezxC"
      },
      "source": [
        "##### 6. Transform the data to implement Kmeans\n",
        "    : make a list of df['kmeans_point']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV1-KkOmT1Oa"
      },
      "outputs": [],
      "source": [
        "#make a dataset for kmeans\n",
        "#p_lists: concat all tokens location point\n",
        "p_lists = []\n",
        "for element in df['kmeans_point']:\n",
        "    for token in element:\n",
        "        p_lists.append(token)\n",
        "\n",
        "k_rdd = sc.parallelize(p_lists)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByQqEffSfRbq"
      },
      "source": [
        "##### 7. Implement Kmeans with proper THRESHOLD and MAXITER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJ4l8nXNT1Oc"
      },
      "outputs": [],
      "source": [
        "#implement kmeans with maximum iteration(MAX_ITER) and stop point(TRESHOLD)\n",
        "THRESHOLD = 0.1\n",
        "MAX_ITER = 1000\n",
        "result_rdd = Kmeans(p_lists)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL5c9VsrLwd3"
      },
      "source": [
        "##### 8. confirm the cluster of every row: df['final_cluster']\n",
        "    : confirmed cluster: mode value of tokens in a row\n",
        "##### 9. save the frequency of tokens in every row: df['token_cluster']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0wHxEkKQLcK"
      },
      "outputs": [],
      "source": [
        "#group the token location points by cino_index\n",
        "result_list = result_rdd.map(lambda t: (int(t[1][0]), t[0])).groupByKey().mapValues(list).collect()\n",
        "\n",
        "#find the mode cluster of every row\n",
        "#count_lists: frequency of tokens in every row\n",
        "#mode_lists: mode cluster number of every row\n",
        "count_lists = []\n",
        "mode_lists = []\n",
        "for element in result_list:\n",
        "  mode_dict = Counter(element[1])\n",
        "  temp = mode_dict.most_common()\n",
        "  count_lists.append(temp)\n",
        "  mode_lists.append(temp[0][0])\n",
        "\n",
        "df['final_clu'] = mode_lists\n",
        "df['tokens_clu'] = count_lists"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu-32TmmkaB3"
      },
      "source": [
        "#### PageRank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F-zXzuxtG3_"
      },
      "source": [
        "##### 1. Define functions\n",
        "    1) Compute LinkStructure by blocks(make_link_set)   \n",
        "    2) Compute PageRank by blocks(pr_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgZ6aTtQ1pS4"
      },
      "outputs": [],
      "source": [
        "def make_link_set(dest_set, block_cnt, block_size):\n",
        "  source = dest_set[0]\n",
        "  block_based_dest_set = [(source, []) for i in range(block_cnt)]\n",
        "\n",
        "  for dest in dest_set[1]:\n",
        "    block_based_dest_set[math.ceil(dest/block_size)-1][1].append(dest)\n",
        "\n",
        "  for i in range(block_cnt):\n",
        "    if len(block_based_dest_set[i][1]) != 0:\n",
        "      yield (i, block_based_dest_set[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqQE-II-1uc9"
      },
      "outputs": [],
      "source": [
        "def pr_map(block_data, block_size, N):\n",
        "  block_id, link_sets = block_data\n",
        "  new_sr = np.zeros(block_size) + 1.5/N\n",
        "\n",
        "  for link_set in link_sets:\n",
        "    source, dest_set = link_set\n",
        "    val = pr.value[source] / len(dest_set)\n",
        "\n",
        "    for dest in dest_set:\n",
        "      new_sr[math.ceil(dest/block_cnt)-1] += val\n",
        "\n",
        "  yield (block_id, new_sr.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD-vK--FmtUI"
      },
      "source": [
        "##### 2. Transform the data to compute PageRank\n",
        "    1) Make a dataframe about cluster member: df_cluster\n",
        "    2) Define function that extract specific cluster's rows: extract_proper_row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AvyoOFbT1Od"
      },
      "outputs": [],
      "source": [
        "#make dataframe about cluster member (df_cluster)\n",
        "df_clu = df.groupby('final_clu')['cino'].apply(list).reset_index(name='clu_elements')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YjZ0o57nB5w"
      },
      "outputs": [],
      "source": [
        "#extract rows which is 'the' cluster member from df\n",
        "#return the particular rows in df\n",
        "def extract_proper_row(clu_elements, df):\n",
        "  df_temp = df[df['cino'] == 'AAA0000']\n",
        "  for element in clu_elements:\n",
        "    df_temp = pd.concat([df_temp, df[df['cino']==element]])\n",
        "  return df_temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVw1LX5ni39D"
      },
      "source": [
        "##### 3. Implement PageRank of every cluster with proper block_cnt and maxIter\n",
        "    1) Make a dataframe with specific cluster's rows: df_row\n",
        "    2) Map (source, dest)pair make\n",
        "        source: temp index of cino (df['temp_index'])   \n",
        "        dest: token_index\n",
        "    3) Build a link structure in blocks\n",
        "    4) Compute PageRank in blocks\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eonrqkeLhWWD"
      },
      "outputs": [],
      "source": [
        "#define parameters for implementing PageRank\n",
        "N = len(token_dict)\n",
        "\n",
        "block = [16, 8, 4, 2, 1]\n",
        "maxIter = 20\n",
        "execution_time = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MhY8yJAgmaq"
      },
      "outputs": [],
      "source": [
        "pr_ranks = []\n",
        "for cluster in range(df_clu.shape[0]):\n",
        "  times = []\n",
        "\n",
        "  #get proper dataset\n",
        "  df_row = extract_proper_row(df_clu['clu_elements'][cluster], df[['cino', 'token_index', 'final_clu']])\n",
        "\n",
        "  lists = []\n",
        "  for i in range(df_row.shape[0]):\n",
        "    lists.append(i)\n",
        "\n",
        "  df_row['temp_index'] = lists\n",
        "\n",
        "  data_lists = []\n",
        "  for i in df_row.index:\n",
        "    lists = []\n",
        "    dest = df_row['temp_index'][i]\n",
        "    lists.append(dest)\n",
        "    lists = lists + df_row['token_index'][i]\n",
        "    data_lists.append(lists)\n",
        "\n",
        "  #implement pagerank with various block sizes and maxIter\n",
        "  d_rdd = sc.parallelize(data_lists).map(lambda t: (t[0], t[1:]))\n",
        "  for block_cnt in block:\n",
        "    start = time.time()\n",
        "\n",
        "    block_size = math.ceil(N/block_cnt)\n",
        "    N = block_cnt*block_size\n",
        "    pr = sc.broadcast([1./float(N) for _ in range(N)])\n",
        "    block_based_set = d_rdd.flatMap(lambda s: make_link_set(s, block_cnt, block_size)).groupByKey().mapValues(list)\n",
        "\n",
        "    for _ in range(maxIter):\n",
        "      itr = block_based_set.flatMap(lambda b: pr_map(b, block_size, N)).reduce(lambda x, y: x+y)\n",
        "\n",
        "      lists = []\n",
        "      zeros = [0 for _ in range(N)]\n",
        "      count = 0\n",
        "      for i in range (int(len(itr)/2)):\n",
        "        if(itr[2*i]==count):\n",
        "          lists.append(itr[2*i+1])\n",
        "        else:\n",
        "          lists.append(zeros)\n",
        "        count = count + 1\n",
        "\n",
        "      pr_lists = []\n",
        "      for element in lists:\n",
        "        pr_lists = pr_lists + element\n",
        "\n",
        "      pr = sc.broadcast(pr_lists)\n",
        "\n",
        "    end = time.time()\n",
        "    times.append(end-start)\n",
        "\n",
        "  execution_time.append(times)\n",
        "  pr_ranks.append(pr.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEv-X-xcwu_L"
      },
      "source": [
        "##### 4. Apply pagerank score to df['tfidf_edit']: df['fianl_score']   \n",
        "    1) Define a function that updates the df['tfidf_edit'] with the PageRank score\n",
        "    2) Update the df['final_clu'] with new index from clu_dict\n",
        "    3) Make a list of (token, pr_edit_score)tuples dictionary for applying PageRank score to data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1i-1eN4UUcE"
      },
      "outputs": [],
      "source": [
        "def pr_update (tfidf_sum_list, weight_dict):\n",
        "    results = []\n",
        "    for i in range(len(tfidf_sum_list)):\n",
        "      temp = list(tfidf_sum_list[i])\n",
        "      temp[1] = temp[1] * weight_dict[i][1]\n",
        "      token_result = tuple(temp)\n",
        "      results.append(token_result)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlPAUgxlPtE-"
      },
      "outputs": [],
      "source": [
        "#update final cluster with new index from clu_dict\n",
        "clu_dict = {old_clu:i for i, old_clu in enumerate(df_clu['final_clu'])}\n",
        "\n",
        "lists = []\n",
        "for element in df['final_clu']:\n",
        "  lists.append(clu_dict[element])\n",
        "df['final_clu'] = lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4DPvS2ejgcx"
      },
      "outputs": [],
      "source": [
        "#make a list of (token, pr_edit_score) tuples dictionary\n",
        "#pr_edit_score: pr_score*100 + 1\n",
        "\n",
        "pr_dict = {v:k for k,v in token_dict.items()}\n",
        "df_pr = pd.DataFrame(pr_ranks)\n",
        "\n",
        "#clu_prs, lists is just for seeing the actual score of pagerank\n",
        "clu_prs = []\n",
        "edit_update = []\n",
        "for element in df_pr.iloc:\n",
        "  lists = []\n",
        "  edit_lists = []\n",
        "  for i in range(len(pr_dict)):\n",
        "    if element[i] > min(element):\n",
        "      lists.append((pr_dict[i], element[i]))\n",
        "      edit_lists.append((pr_dict[i], element[i]*1000 + 1))\n",
        "  clu_prs.append(lists)\n",
        "  edit_update.append(dict(edit_lists))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvRGf97x4lVK"
      },
      "outputs": [],
      "source": [
        "#trans\n",
        "final_score = []\n",
        "for i in range(df.shape[0]):\n",
        "  w_lists = []\n",
        "  for token in df['compound_nouns'][i]:\n",
        "    if token in edit_update[df['final_clu'][i]]:\n",
        "      w_lists.append((token, edit_update[df['final_clu'][i]][token]))\n",
        "    else:\n",
        "      w_lists.append((token, 1))\n",
        "  final_score.append(pr_update(df['tfidf_edit'][i], w_lists))\n",
        "\n",
        "df['final_score'] = final_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTtbxDFrXkN1"
      },
      "source": [
        "#### Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvWBt7fsXz6p"
      },
      "outputs": [],
      "source": [
        "k_lists = []\n",
        "for element in df['final_score']:\n",
        "    k_lists.append(sorted(element, key =lambda t: t[1], reverse=True))\n",
        "df['sorted_token'] = k_lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFBRAPXSXz6w",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "lists = []\n",
        "c_lists = []\n",
        "for i in range(df.shape[0]):\n",
        "    if (len(df.iloc[i]['sorted_token'])>10):\n",
        "        for j in range(10):\n",
        "            c_lists.append(df.iloc[i]['cino'])\n",
        "            lists.append(df.iloc[i]['sorted_token'][j][0])\n",
        "    elif(len(df.iloc[i]['sorted_token'])>0):\n",
        "        for j in range(len(df.iloc[i]['sorted_token'])):\n",
        "            c_lists.append(df.iloc[i]['cino'])\n",
        "            lists.append(df.iloc[i]['sorted_token'][j][0])\n",
        "    else:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wt6nZS2Xz6w"
      },
      "outputs": [],
      "source": [
        "df_result = pd.DataFrame(c_lists)\n",
        "df_result['aa'] = lists\n",
        "df_result.columns = ['cino', 'keyword']\n",
        "\n",
        "df_result.to_csv('final_keyword.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyLzFBYaXz6w"
      },
      "outputs": [],
      "source": [
        "df_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F66NIBaKGxs3"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2OhVCWeGkov"
      },
      "outputs": [],
      "source": [
        "df.to_csv('test_pr_result.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "s4CPw-8aOQFT",
        "QAlygjekOn94",
        "x_ro9XD8aXme",
        "gWY9WsqcUv6B",
        "ZWrsqnamU0rp",
        "-8tvxgWdVYUJ",
        "pSmLPOq0NS-M",
        "yqKzjb2BM-Sc",
        "JonXWmVWVoQ5",
        "GC6FF-UIZNSo",
        "WWLqxZldWHSx",
        "vWhULD1JezxC",
        "bL5c9VsrLwd3",
        "lu-32TmmkaB3",
        "1F-zXzuxtG3_",
        "aD-vK--FmtUI",
        "FVw1LX5ni39D",
        "IEv-X-xcwu_L",
        "t0v_nDUMCWtS",
        "_XrthB5r-mI0",
        "bT_sFlmotCh_",
        "tmll8wS0OE54"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}